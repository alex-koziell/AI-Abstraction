{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense (Fully Connected) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exports.e_01_testing as tst\n",
    "import exports.e_02_MNISTLoader as ldr\n",
    "\n",
    "import math\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s):\n",
    "    \"\"\"normalize x, with m = mean, s = std dev \"\"\"\n",
    "    return (x - m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = ldr.loadMNIST()\n",
    "\n",
    "x_train = normalize(x_train, x_train.mean(), x_train.std())\n",
    "# NB: Use training, not validation mean for validation set\n",
    "# (model parameters are sensitive to the normalization parameters - mean and stddev)\n",
    "x_valid = normalize(x_valid, x_train.mean(), x_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_inputs = x_train.shape\n",
    "n_outputs = 1\n",
    "n_neurons = 50\n",
    "\n",
    "# simplified Kaiming init / He init\n",
    "w1 = torch.randn(n_inputs, n_neurons)/math.sqrt(n_inputs)\n",
    "b1 = torch.zeros(n_neurons)\n",
    "w2 = torch.randn(n_neurons, n_outputs) / math.sqrt(n_neurons)\n",
    "b2 = torch.zeros(n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "Layers (linear, activation and loss layers) will be implemented as subclasses of an abstract class `Module`.\n",
    "\n",
    "The final layer will always be a loss layer to make a backwards pass possible. This is because loss layer gradients do not depend on output gradients fed back to the layer. Hence `(value).grad` means the gradient of the loss wrt value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    \"\"\" Abstract class that is capable of a forward and backward pass\n",
    "        on some inputs to produce and output and accumulate gradients. \"\"\"\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out  = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('Not Implemented')\n",
    "    def backward(self): raise Exception('Not Implemented')\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\" Linear layer; out = weights.in + bias \"\"\"\n",
    "    \n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights, self.bias = weights, bias\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp@self.weights + self.bias\n",
    "    \n",
    "    def backward(self, inp):\n",
    "        inp.grd = self.out.grd @ self.weights.t()\n",
    "        self.weights.grd = inp.t() @ self.out.grd\n",
    "        self.bias.grd = self.out.grd.sum()\n",
    "        \n",
    "class ReLU(Module):\n",
    "    \"\"\" Rectified linear unit activation layer. \"\"\"\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.)\n",
    "    \n",
    "    def backward(self, inp):\n",
    "        inp.grd = (inp > 0).float() * self.out.grd\n",
    "        \n",
    "class MSE(Module):\n",
    "    \"\"\" Mean squared error. \"\"\"\n",
    "    \n",
    "    def forward(self, inp, labels):\n",
    "        return (inp.squeeze() - labels).pow(2).mean()\n",
    "    \n",
    "    def backward(self, inp, labels):\n",
    "        inp.grd = (inp.squeeze() - labels).unsqueeze(-1) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self, params):\n",
    "        self.layers = [Linear(params['w1'], params['b1']),\n",
    "                       ReLU(),\n",
    "                       Linear(params['w2'], params['b2']),\n",
    "                       MSE()]\n",
    "        self.lossLayer = self.layers[-1]\n",
    "        \n",
    "    def __call__(self, x, labels):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        return self.lossLayer(x, labels)\n",
    "    \n",
    "    def backward(self):\n",
    "        (x, labels) = self.lossLayer.args\n",
    "        self.lossLayer.backward(x, labels)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            layer.backward(layer.args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 28.699634552001953\n",
      "torch.Size([784, 50])\n"
     ]
    }
   ],
   "source": [
    "# Normalized\n",
    "inp = x_train\n",
    "lab = y_train\n",
    "# Kaiming initialized\n",
    "params = {'w1': w1, 'b1': b1,'w2': w2, 'b2': b2}\n",
    "\n",
    "dnn = DNN(params)\n",
    "mse = dnn(inp, lab)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "dnn.backward()\n",
    "print(dnn.layers[0].weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "from torch import nn\n",
    "\n",
    "class TorchDNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons, n_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_inputs, n_neurons, n_outputs),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(n_neurons, n_outputs)]\n",
    "        self.lossLayer = nn.MSELoss()\n",
    "        \n",
    "    def __call__(self, x, labels):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.loss = self.lossLayer(x.squeeze(), labels)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 27.24370574951172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels are of type long... convert to float\n",
    "y_train_float = y_train.float()\n",
    "\n",
    "torchDNN = TorchDNN(n_inputs, n_neurons, n_outputs)\n",
    "loss = torchDNN(x_train, y_train_float)\n",
    "print(f'MSE: {loss}')\n",
    "torchDNN.backward()\n",
    "torchDNN.layers[0].weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
