{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exports.e_01_testing import test_near_torch\n",
    "from exports.e_02_MNISTLoader import loadMNIST\n",
    "from exports.e_04_DataAPI import Dataset\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_inp, n_hid, n_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inpL = nn.Linear(n_inp, n_hid)\n",
    "        self.hidL = nn.ReLU(n_hid)\n",
    "        self.outL = nn.Linear(n_hid, n_out)\n",
    "        \n",
    "        self.layers = [self.inpL, self.hidL, self.outL]\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = loadMNIST()\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "n_sampl, n_inp = x_train.shape\n",
    "n_out = 10\n",
    "n_hid = 50\n",
    "\n",
    "model = Model(n_inp, n_hid, n_out)\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax, Negative Log Likelihood and Cross Entropy Loss\n",
    "Since the labels will be one-hot encoded, we get the NLL by clever use of indexing.\n",
    "\n",
    "### LogSumExp Trick\n",
    "\n",
    "For numerical stability\n",
    "\n",
    "$ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2961, -2.3437, -2.4982,  ..., -2.3744, -2.2866, -2.3065],\n",
       "        [-2.3001, -2.3351, -2.4629,  ..., -2.3046, -2.2995, -2.3802],\n",
       "        [-2.2967, -2.3184, -2.4669,  ..., -2.3209, -2.2645, -2.5148],\n",
       "        ...,\n",
       "        [-2.2160, -2.2770, -2.4233,  ..., -2.3545, -2.3835, -2.4213],\n",
       "        [-2.1351, -2.3462, -2.3826,  ..., -2.4427, -2.3572, -2.4290],\n",
       "        [-2.2223, -2.3551, -2.4704,  ..., -2.2734, -2.4102, -2.4339]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logSumExp(x): m = x.max(-1)[0]; return m + (x - m[:, None]).exp().sum(-1).log()\n",
    "\n",
    "def softmax(x): return x.exp() / x.exp().sum(-1, keepdim=True)\n",
    "def log_softmax(x): return x - logSumExp(x)[:, None]\n",
    "\n",
    "sm_pred = log_softmax(pred)\n",
    "sm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(x, lab): return -x[:, lab].mean()\n",
    "\n",
    "loss = NLL(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Pytorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3391],\n",
       "        [2.3298],\n",
       "        [2.3209],\n",
       "        ...,\n",
       "        [2.2965],\n",
       "        [2.3271],\n",
       "        [2.2889]], grad_fn=<LogsumexpBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3124, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3124, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(F.log_softmax(pred, -1), y_train), F.cross_entropy(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments ARE near.\n"
     ]
    }
   ],
   "source": [
    "test_near_torch(F.nll_loss(F.log_softmax(pred, -1), y_train), F.cross_entropy(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers and Training\n",
    "\n",
    "Basic structure:\n",
    "\n",
    "1. Feed Forward: Compute outputs on a set of inputs.\n",
    "2. Compute loss from outputs and labels.\n",
    "3. Backpropagate: compute the gradients of the loss with respect to each model parameter.\n",
    "4. Update: update parameters using the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_func(pred, lab): return (torch.argmax(pred, dim=1) == lab).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.294121265411377\n",
      "Accuracy: 0.15625\n"
     ]
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64            # batch size\n",
    "inp = x_train[:bs]\n",
    "lab = y_train[:bs]\n",
    "pred = model(inp)\n",
    "print(f'Loss: {loss_func(pred, lab)}\\nAccuracy: {acc_func(pred, lab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "We'll use gradient descent as our optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, learning_rate, params): self.lr, self.params = learning_rate, list(params)\n",
    "        \n",
    "    def step(self):\n",
    "        # Gradient Descent\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs, loss_func, acc_func, optimizer, train_dl, valid_dl):\n",
    "    tot_loss, tot_acc = 0., 0.\n",
    "    n_valid = len(valid_dl)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()         # SGD\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += acc_func(pred, yb)\n",
    "            print(f'Epoch: {epoch+1}, Loss:{tot_loss/n_valid}\\t Acc:{tot_acc/n_valid}')\n",
    "            \n",
    "    return tot_loss/n_valid, tot_acc/n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:0.2933460772037506\t Acc:0.9052547812461853\n",
      "Epoch: 2, Loss:0.1334770768880844\t Acc:0.9629777073860168\n",
      "Epoch: 3, Loss:0.13553813099861145\t Acc:0.9596934914588928\n",
      "Epoch: 4, Loss:0.11354310065507889\t Acc:0.9686504602432251\n",
      "Epoch: 5, Loss:0.4481799304485321\t Acc:0.8923168778419495\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)\n",
    "opt = Optimizer(0.3, model.parameters())\n",
    "\n",
    "loss, acc = fit(model, 5, loss_func, acc_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Implementations and Cleanup\n",
    "\n",
    "`nn.Sequential` allows us to define a model consisting of a sequence of layers as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(n_inp, n_hid), nn.ReLU(n_hid), nn.Linear(n_hid, n_out))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.optim` gives a SGD optimizer similar to the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.3\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.3)\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear and concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_dls(train_ds, valid_ds, batch_size, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:0.18260787427425385\t Acc:0.9467958807945251\n",
      "Epoch: 2, Loss:0.16307620704174042\t Acc:0.9539161324501038\n",
      "Epoch: 3, Loss:0.11055906862020493\t Acc:0.968156635761261\n",
      "Epoch: 4, Loss:0.10845605283975601\t Acc:0.968156635761261\n",
      "Epoch: 5, Loss:0.10642056167125702\t Acc:0.9708267450332642\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "train_dl, valid_dl = make_dls(train_ds, valid_ds, 64)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(nn.Linear(n_inp, n_hid), nn.ReLU(n_hid), nn.Linear(n_hid, n_out))\n",
    "opt = opt = optim.SGD(model.parameters(), lr=0.3)\n",
    "\n",
    "# training and eval\n",
    "loss, acc = fit(model, 5, loss_func, acc_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 05_Losses_Optimizers_TrainEval.ipynb has been converted to module ./exports/e_05_Losses_Optimizers_TrainEval.py!\r\n"
     ]
    }
   ],
   "source": [
    "!python utils/export_notebook.py 05_Losses_Optimizers_TrainEval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
