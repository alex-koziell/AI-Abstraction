{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "from exports.e_07_Annealing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def normalize(x, m, s): return (x-m)/s\n",
    "\n",
    "def normalize_data(train, valid):\n",
    "    m,s = train.mean(),train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def MNISTDataWrapper():\n",
    "    x_train, y_train, x_valid, y_valid = loadMNIST()\n",
    "    x_train, x_valid = normalize_data(x_train, x_valid)\n",
    "\n",
    "    train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "    n_sampl, n_inp = x_train.shape\n",
    "    n_out = 10\n",
    "    n_hid = 50\n",
    "\n",
    "    batch_size = 512\n",
    "\n",
    "    return DataWrapper(*make_dls(train_ds, valid_ds, batch_size), n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w = MNISTDataWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Layer\n",
    "This will allow us to convert the MNIST data between square and flat formats, which will allow us to use a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "        \n",
    "def flatten(x): return x.view(x.shape[0], -1)\n",
    "def mnist_square(x): return x.view(-1 , 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def CNNModel(data_w, lr=0.3):\n",
    "    n_inp, n_out = data_w.train_ds.x.shape[1], data_w.n_out\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        Lambda(mnist_square),\n",
    "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), # 14\n",
    "        nn.Conv2d( 8,16, 3, padding=2,stride=2), nn.ReLU(), # 7\n",
    "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
    "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),\n",
    "        nn.Linear(32,n_out)\n",
    "    )\n",
    "    \n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = ModelWrapper(*CNNModel(data_w), F.cross_entropy, data_w)\n",
    "cbs = [AvgStatsCB([acc_f])]\n",
    "job = DLJob(cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs are much slower, but more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.9277984375, tensor(0.3548)]\n",
      "valid: [0.72882373046875, tensor(0.7561)]\n",
      "train: [0.449520546875, tensor(0.8605)]\n",
      "valid: [0.25144990234375, tensor(0.9257)]\n",
      "train: [0.2117690625, tensor(0.9358)]\n",
      "valid: [0.15630604248046875, tensor(0.9542)]\n",
      "train: [0.1521771875, tensor(0.9543)]\n",
      "valid: [0.1216452392578125, tensor(0.9636)]\n",
      "train: [0.125634755859375, tensor(0.9612)]\n",
      "valid: [0.11315538330078125, tensor(0.9665)]\n",
      "train: [0.10199859375, tensor(0.9692)]\n",
      "valid: [0.13249073486328125, tensor(0.9588)]\n",
      "train: [0.091045458984375, tensor(0.9721)]\n",
      "valid: [0.15326160888671875, tensor(0.9544)]\n",
      "train: [0.0812145361328125, tensor(0.9752)]\n",
      "valid: [0.0932900634765625, tensor(0.9714)]\n",
      "3.14 s ± 345 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit job.fit(1, model_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 08_LambdaLayers.ipynb has been converted to module ./exports/e_08_LambdaLayers.py!\r\n"
     ]
    }
   ],
   "source": [
    "!python utils/export_notebook.py 08_LambdaLayers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
