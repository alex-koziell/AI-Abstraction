{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "from exports.e_07_Annealing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def normalize(x, m, s): return (x-m)/s\n",
    "\n",
    "def normalize_data(train, valid):\n",
    "    m,s = train.mean(),train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def MNISTDataWrapper():\n",
    "    x_train, y_train, x_valid, y_valid = loadMNIST()\n",
    "    x_train, x_valid = normalize_data(x_train, x_valid)\n",
    "\n",
    "    train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "    n_sampl, n_inp = x_train.shape\n",
    "    n_out = 10\n",
    "    n_hid = 50\n",
    "\n",
    "    batch_size = 512\n",
    "\n",
    "    return DataWrapper(*make_dls(train_ds, valid_ds, batch_size), n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w = MNISTDataWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Layer\n",
    "This will allow us to convert the MNIST data between square and flat formats, which will allow us to use a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "        \n",
    "def flatten(x): return x.view(x.shape[0], -1)\n",
    "def mnist_square(x): return x.view(-1 , 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "def CNNModel(data_w, lr=0.3):\n",
    "    n_inp, n_out = data_w.train_ds.x.shape[1], data_w.n_out\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        Lambda(mnist_square),\n",
    "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), # 14\n",
    "        nn.Conv2d( 8,16, 3, padding=2,stride=2), nn.ReLU(), # 7\n",
    "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
    "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),\n",
    "        nn.Linear(32,n_out)\n",
    "    )\n",
    "    \n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = ModelWrapper(*CNNModel(data_w), F.cross_entropy, data_w)\n",
    "cbs = [AvgStatsCB([acc_f])]\n",
    "job = DLJob(cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs are much slower, but more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.0539221875, tensor(0.2785)]\n",
      "valid: [0.8967267578125, tensor(0.7219)]\n",
      "train: [0.49458296875, tensor(0.8466)]\n",
      "valid: [0.2300808837890625, tensor(0.9323)]\n",
      "train: [0.2500250390625, tensor(0.9246)]\n",
      "valid: [0.16810185546875, tensor(0.9511)]\n",
      "train: [0.153403525390625, tensor(0.9535)]\n",
      "valid: [0.14906611328125, tensor(0.9534)]\n",
      "train: [0.11439572265625, tensor(0.9653)]\n",
      "valid: [0.1084128662109375, tensor(0.9677)]\n",
      "train: [0.096074375, tensor(0.9708)]\n",
      "valid: [0.10416944580078125, tensor(0.9681)]\n",
      "train: [0.084914716796875, tensor(0.9735)]\n",
      "valid: [0.08003513793945312, tensor(0.9759)]\n",
      "train: [0.0735136865234375, tensor(0.9772)]\n",
      "valid: [0.08957796020507812, tensor(0.9736)]\n",
      "3.7 s ± 173 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit job.fit(1, model_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "class CudaCB(Callback):\n",
    "    def __init__(self, device): self.device = device\n",
    "    def begin_fit(self): self.model.to(self.device)\n",
    "    def begin_batch(self): self.job.xb, self.job.yb = self.xb.to(self.device), self.yb.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [AvgStatsCB([acc_f]),\n",
    "       CudaCB(torch.device('cuda', 0))]\n",
    "job = DLJob(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately my GPU is too old\n",
    "# %timeit job.fit(1, model_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transform Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--export--#\n",
    "class BatchTransformCB(Callback):\n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "        \n",
    "    def begin_batch(self): self.job.xb = self.tfm(self.xb)\n",
    "\n",
    "def view_tfm(size): \n",
    "    def _inner(x) : return x.view(*((-1,)+size))\n",
    "    return _inner\n",
    "\n",
    "mnist_view = view_tfm((1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [AvgStatsCB([acc_f]),\n",
    "       BatchTransformCB(mnist_view)]\n",
    "job = DLJob(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.29350875, tensor(0.1319)]\n",
      "valid: [2.2261279296875, tensor(0.3111)]\n"
     ]
    }
   ],
   "source": [
    "job.fit(1, model_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 08_LambdaLayers.ipynb has been converted to module ./exports/e_08_LambdaLayers.py!\r\n"
     ]
    }
   ],
   "source": [
    "!python utils/export_notebook.py 08_LambdaLayers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
